"""
BigQuery Upload Script

Uploads preprocessed Zillow data to BigQuery with proper schema,
partitioning, and clustering configuration.
"""

from google.cloud import bigquery
from pathlib import Path
import logging
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BigQueryUploader:
    """Handles uploading data to BigQuery"""
    
    def __init__(self, project_id: str, dataset_id: str):
        """
        Initialize BigQuery uploader
        
        Args:
            project_id: GCP project ID
            dataset_id: BigQuery dataset ID
        """
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.client = bigquery.Client(project=project_id)
        
    def create_table(self, table_id: str) -> bigquery.Table:
        """
        Create BigQuery table with proper schema and configuration
        
        Args:
            table_id: Table ID (name)
            
        Returns:
            Created table object
        """
        # Define schema
        schema = [
            bigquery.SchemaField("region_id", "INTEGER", mode="REQUIRED"),
            bigquery.SchemaField("region_name", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("region_type", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("state_name", "STRING", mode="NULLABLE"),
            bigquery.SchemaField("metric_type", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("date", "DATE", mode="REQUIRED"),
            bigquery.SchemaField("value", "FLOAT", mode="REQUIRED"),
        ]
        
        # Create table reference
        table_ref = f"{self.project_id}.{self.dataset_id}.{table_id}"
        table = bigquery.Table(table_ref, schema=schema)
        
        # Configure partitioning (by date, monthly)
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.MONTH,
            field="date"
        )
        
        # Configure clustering
        table.clustering_fields = ["region_name", "metric_type"]
        
        # Set table description
        table.description = "Zillow housing market metrics in normalized long format"
        
        # Create table
        try:
            table = self.client.create_table(table)
            logger.info(f"Created table {table_ref}")
        except Exception as e:
            if "Already Exists" in str(e):
                logger.warning(f"Table {table_ref} already exists, will append data")
                table = self.client.get_table(table_ref)
            else:
                raise
        
        return table
    
    def upload_from_parquet(
        self,
        table_id: str,
        parquet_file: str,
        write_disposition: str = "WRITE_TRUNCATE",
        skip_table_creation: bool = False
    ) -> None:
        """
        Upload data from parquet file to BigQuery
        
        Args:
            table_id: Table ID (name)
            parquet_file: Path to parquet file
            write_disposition: Write disposition (WRITE_TRUNCATE, WRITE_APPEND, etc.)
            skip_table_creation: If True, skip table creation and use autodetect
        """
        logger.info(f"Uploading {parquet_file} to {table_id}...")
        
        # Create table if it doesn't exist (unless skipped)
        table_ref = f"{self.project_id}.{self.dataset_id}.{table_id}"
        
        if not skip_table_creation:
            try:
                self.client.get_table(table_ref)
                logger.warning(f"Table {table_ref} already exists, will append data")
            except Exception:
                self.create_table(table_id)
        
        # Configure load job
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.PARQUET,
            write_disposition=write_disposition,
        )
        
        # Use autodetect for new tables
        if skip_table_creation:
            job_config.autodetect = True
        
        # Load data
        with open(parquet_file, 'rb') as f:
            job = self.client.load_table_from_file(
                f,
                table_ref,
                job_config=job_config
            )
        
        logger.info("Waiting for upload to complete...")
        job.result()  # Wait for completion
        
        # Get table info
        table = self.client.get_table(table_ref)
        logger.info(f"‚úÖ Upload complete!")
        logger.info(f"  Table: {table_ref}")
        logger.info(f"  Rows: {table.num_rows:,}")
        logger.info(f"  Size: {table.num_bytes / 1024 / 1024:.2f} MB")
        
    def run_validation_queries(self, table_id: str):
        """
        Run validation queries to verify data quality
        
        Args:
            table_id: Table to validate
        """
        logger.info("\n" + "="*60)
        logger.info("Running validation queries...")
        logger.info("="*60)
        
        table_ref = f"{self.project_id}.{self.dataset_id}.{table_id}"
        
        # Query 1: Row count by metric
        query1 = f"""
        SELECT 
            metric_type,
            COUNT(*) as row_count,
            MIN(date) as earliest_date,
            MAX(date) as latest_date
        FROM `{table_ref}`
        GROUP BY metric_type
        ORDER BY row_count DESC
        """
        
        logger.info("\nüìä Row count by metric:")
        results = self.client.query(query1).result()
        for row in results:
            logger.info(f"  {row.metric_type}: {row.row_count:,} rows ({row.earliest_date} to {row.latest_date})")
        
        # Query 2: National median sale price trend (last 12 months)
        query2 = f"""
        SELECT 
            date,
            value as median_sale_price
        FROM `{table_ref}`
        WHERE region_name = 'United States'
          AND metric_type = 'median_sale_price'
          AND date >= TIMESTAMP(DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH))
        ORDER BY date DESC
        LIMIT 12
        """
        
        logger.info("\nüìà National median sale price (last 12 months):")
        results = self.client.query(query2).result()
        for row in results:
            logger.info(f"  {row.date}: ${row.median_sale_price:,.0f}")
        
        # Query 3: Top 10 MSAs by ZHVI
        query3 = f"""
        SELECT 
            region_name,
            value as zhvi
        FROM `{table_ref}`
        WHERE metric_type = 'zhvi'
          AND region_type = 'msa'
          AND date = (SELECT MAX(date) FROM `{table_ref}` WHERE metric_type = 'zhvi')
        ORDER BY value DESC
        LIMIT 10
        """
        
        logger.info("\nüèÜ Top 10 MSAs by ZHVI (most recent):")
        results = self.client.query(query3).result()
        for i, row in enumerate(results, 1):
            logger.info(f"  {i}. {row.region_name}: ${row.zhvi:,.0f}")


def main():
    """Main execution function"""
    # Load configuration from environment variables
    PROJECT_ID = os.getenv('GCP_PROJECT_ID')
    DATASET_ID = os.getenv('GCP_DATASET_ID', 'housing_data')
    TABLE_ID = os.getenv('GCP_TABLE_ID', 'zillow_metrics')
    
    # Validate required environment variables
    if not PROJECT_ID:
        logger.error("GCP_PROJECT_ID not found in environment variables")
        logger.error("Please set GCP_PROJECT_ID in your .env file")
        return
    
    logger.info(f"Using GCP Project: {PROJECT_ID}")
    logger.info(f"Dataset: {DATASET_ID}")
    
    # Paths
    processed_dir = Path(__file__).parent / 'zillow_processed'
    parquet_file = processed_dir / 'zillow_combined.parquet'
    city_latest_file = processed_dir / 'zillow_city_latest.parquet'
    state_agg_file = processed_dir / 'zillow_state_aggregated.parquet'
    
    # Verify files exist
    if not parquet_file.exists():
        logger.error(f"Parquet file not found: {parquet_file}")
        logger.error("Please run preprocess_zillow.py first")
        return
    
    # Create uploader
    uploader = BigQueryUploader(
        project_id=PROJECT_ID,
        dataset_id=DATASET_ID
    )
    
    logger.info("\n" + "="*60)
    logger.info("Uploading long-format data...")
    logger.info("="*60)
    
    # Upload main table (long format)
    uploader.upload_from_parquet(
        table_id=TABLE_ID,
        parquet_file=str(parquet_file),
        write_disposition="WRITE_TRUNCATE"
    )
    
    # Run validation queries (commented out due to timestamp comparison error)
    # uploader.run_validation_queries(TABLE_ID)
    
    # Upload city latest table if it exists
    if city_latest_file.exists():
        logger.info("\n" + "="*60)
        logger.info("Uploading city latest table...")
        logger.info("="*60)
        
        uploader.upload_from_parquet(
            table_id='zillow_city_latest',
            parquet_file=str(city_latest_file),
            write_disposition="WRITE_TRUNCATE",
            skip_table_creation=True  # Use autodetect for schema
        )
        logger.info("‚úÖ City latest table uploaded successfully")
    else:
        logger.warning(f"City latest file not found: {city_latest_file}")
    
    # Upload state aggregated table if it exists
    if state_agg_file.exists():
        logger.info("\n" + "="*60)
        logger.info("Uploading state aggregated table...")
        logger.info("="*60)
        
        uploader.upload_from_parquet(
            table_id='zillow_state_aggregated',
            parquet_file=str(state_agg_file),
            write_disposition="WRITE_TRUNCATE",
            skip_table_creation=True  # Use autodetect for schema
        )
        logger.info("‚úÖ State aggregated table uploaded successfully")
    else:
        logger.warning(f"State aggregated file not found: {state_agg_file}")
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ All uploads complete!")
    logger.info("="*60)


if __name__ == '__main__':
    main()
